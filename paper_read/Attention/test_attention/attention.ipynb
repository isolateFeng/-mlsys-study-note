{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttension(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
    "    def __init__(self, scale):\n",
    "        super().__init__() #声明父类的Init方法\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim = 2) #沿哪一维实施softmax\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        #TORCH.BMM 执行batch内两矩阵乘积运算：bmm(b*n*m, b*m*p) -> size(b*n*p)\n",
    "        #TORCH.BMM 输入必须是3-dim tensor\n",
    "        # 1.score = q \\cdot k ，使用query和k点乘（matmul）获得相关度\n",
    "        u = torch.bmm(q, k.transpose(1, 2))\n",
    "        # 2.缩放\n",
    "        u = u / self.scale \n",
    "        # 3.mask(opt)\n",
    "        if mask is not None:\n",
    "            u = u.masked_fill(mask, -np.inf)\n",
    "        # 4.softmax\n",
    "        attn = self.softmax(u)\n",
    "        # 每个key都是等权\n",
    "        output = torch.bmm(attn, v) # attn是每个key对于query的权重，v是每个key对应的value\n",
    "        return attn, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 个数 \n",
    "# n_q ?= n_k == n_v\n",
    "n_q, n_k, n_v = 2, 4, 4\n",
    "# 维度\n",
    "# d_q_ == d_k_ ?= d_v_\n",
    "d_q_, d_k_, d_v_ = 128, 128, 64\n",
    "batch = 32\n",
    "\n",
    "q = torch.randn(batch, n_q, d_q_) # batch, 个数 : n_q, 维度 d_q_\n",
    "k = torch.randn(batch, n_k, d_k_)\n",
    "v = torch.randn(batch, n_v, d_v_)\n",
    "mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "# print(mask)\n",
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "attension = ScaledDotProductAttension(scale=np.power(d_k_, 0.5)) # 实例化\n",
    "attn, output = attension(q, k, v, mask = mask) # 调用函数\n",
    "\n",
    "# print(attn); \n",
    "# print(output)\n",
    "print(attn.size()) # (batch, n_q, n_k) 表示每个k相对于q的重要程度\n",
    "print(output.size()) # (batch, n_q, d_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention \"\"\"\n",
    "    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k # d_k == d_q\n",
    "        self.d_v = d_v\n",
    "\n",
    "        # linear(in, out) --> out = A * in + B\n",
    "        # 线性变换，将(batch, d_k_)大小的tensor变换为(batch, n_head * d_k)大小的tensor\n",
    "        # 多头中，输入为 d_k_ 维度的 k --> 相当于 n_head 个 输入为 d_k 维度的 k\n",
    "        # d_k_ \\ n_head \\ d_k 都由人为设定（作为超参数传入）\n",
    "        self.fc_q = nn.Linear(d_k_, n_head * d_k) \n",
    "        self.fc_k = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_v = nn.Linear(d_v_, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttension(scale=np.power(d_k, 0.5))\n",
    "        self.fc_concatOutput = nn.Linear(n_head * d_v, d_o) # concat -> mlp -> output\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
    "        batch, n_q, d_q_ = q.size()\n",
    "        batch, n_k, d_k_ = k.size()\n",
    "        batch, n_v, d_v_ = v.size()\n",
    "\n",
    "        #投影变化，单头变多头\n",
    "        q = self.fc_q(q) # (batch, n_q, d_q_) -> n_head * (batch, n_q, d_q)\n",
    "        k = self.fc_k(k)\n",
    "        v = self.fc_v(v)\n",
    "\n",
    "        # view method1: (batch, n_q, n_head * d_q) -> (batch, n_q, n_head, d_q)\n",
    "        # permute method: 将tensor维度重排列为 (n_head, batch, n_q, d_q)\n",
    "        # contiguous method: 确保张量在内存中是连续存储的\n",
    "        # view method2: (n_head, batch, n_q, d_q) -> (n_head * batch, n_q, d_q)\n",
    "        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
    "        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)\n",
    "        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)\n",
    "\n",
    "        if mask is not None:\n",
    "            # repeat(n_head, 1, 1): 将mask沿第0维复制 n_head次，其他维度不变\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        attn, output = self.attention(q, k, v, mask=mask) # 当成单头注意力求输出\n",
    "\n",
    "        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1)  # Concat\n",
    "        output = self.fc_concatOutput(output)  # 投影变换得到最终输出\n",
    "        return attn, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 2, 4])\n",
      "torch.Size([32, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(n_head=8, d_k_=128, d_v_=64, d_k=256, d_v=128, d_o=128)\n",
    "attn, output = mha(q, k, v, mask=mask)\n",
    "\n",
    "# print(attn); print(output)\n",
    "print(attn.size()) # [256, 2, 4], 256=batch*n_head, n_q, n_k\n",
    "print(output.size()) # batch, n_q, d_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" Self-Attention \"\"\"\n",
    "    def __init__(self, n_head, d_k, d_v, d_x, d_o):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))\n",
    "\n",
    "        self.mha = MultiHeadAttention(n_head=n_head, d_k_=d_k, d_v_=d_v, d_k=d_k, d_v=d_v, d_o=d_o)\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            stdv = 1. / np.power(param.size(-1), 0.5)\n",
    "            param.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        q = torch.matmul(x, self.wq) # (batch, (4, 80) * (80, 128))\n",
    "        k = torch.matmul(x, self.wk) # (batch, (4, 80) * (80, 128))\n",
    "        v = torch.matmul(x, self.wv) # (batch, (4, 80) * (80, 64))\n",
    "\n",
    "        attn, output = self.mha(q, k, v, mask=mask)\n",
    "        return attn, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2232, 0.2756, 0.2380, 0.2632],\n",
      "         [0.2449, 0.2289, 0.2621, 0.2641],\n",
      "         [0.2487, 0.2416, 0.2542, 0.2555],\n",
      "         [0.2515, 0.2345, 0.2561, 0.2579]],\n",
      "\n",
      "        [[0.2191, 0.2462, 0.2523, 0.2823],\n",
      "         [0.2408, 0.2261, 0.2581, 0.2749],\n",
      "         [0.2656, 0.2309, 0.2353, 0.2682],\n",
      "         [0.2426, 0.2341, 0.2610, 0.2624]],\n",
      "\n",
      "        [[0.2540, 0.2367, 0.2761, 0.2331],\n",
      "         [0.2425, 0.2543, 0.2479, 0.2554],\n",
      "         [0.2609, 0.2374, 0.2567, 0.2450],\n",
      "         [0.2682, 0.2469, 0.2400, 0.2449]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2495, 0.2450, 0.2507, 0.2549],\n",
      "         [0.2494, 0.2584, 0.2461, 0.2461],\n",
      "         [0.2701, 0.2419, 0.2528, 0.2351],\n",
      "         [0.2566, 0.2540, 0.2580, 0.2314]],\n",
      "\n",
      "        [[0.2468, 0.2485, 0.2493, 0.2554],\n",
      "         [0.2422, 0.2286, 0.2621, 0.2671],\n",
      "         [0.2502, 0.2570, 0.2375, 0.2553],\n",
      "         [0.2339, 0.2514, 0.2444, 0.2704]],\n",
      "\n",
      "        [[0.2451, 0.2650, 0.2393, 0.2507],\n",
      "         [0.2559, 0.2283, 0.2347, 0.2812],\n",
      "         [0.2506, 0.2616, 0.2528, 0.2350],\n",
      "         [0.2541, 0.2682, 0.2352, 0.2426]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-2.9501e-01,  5.6252e-02,  1.5794e-01,  ...,  1.1963e-01,\n",
      "           1.2269e-01,  2.0829e-01],\n",
      "         [-3.0157e-01,  4.4497e-02,  1.5444e-01,  ...,  1.0583e-01,\n",
      "           1.1768e-01,  1.9995e-01],\n",
      "         [-3.0508e-01,  4.5567e-02,  1.3991e-01,  ...,  9.9480e-02,\n",
      "           1.2901e-01,  1.9957e-01],\n",
      "         [-2.9808e-01,  5.2579e-02,  1.6063e-01,  ...,  1.0222e-01,\n",
      "           1.2561e-01,  2.0315e-01]],\n",
      "\n",
      "        [[-1.0680e-01,  1.2823e-01, -1.5029e-02,  ..., -6.5054e-02,\n",
      "           1.1900e-01,  1.7026e-01],\n",
      "         [-1.1054e-01,  1.3200e-01, -1.4364e-02,  ..., -6.8947e-02,\n",
      "           1.0781e-01,  1.6480e-01],\n",
      "         [-1.1057e-01,  1.3597e-01, -1.8062e-02,  ..., -7.4535e-02,\n",
      "           1.1186e-01,  1.6543e-01],\n",
      "         [-1.0881e-01,  1.2507e-01, -1.4169e-02,  ..., -7.3165e-02,\n",
      "           1.0460e-01,  1.5999e-01]],\n",
      "\n",
      "        [[-2.8175e-02,  1.9469e-01,  2.7718e-02,  ..., -1.7134e-01,\n",
      "           1.3835e-01,  1.4502e-01],\n",
      "         [-3.4871e-02,  2.0422e-01,  2.5709e-02,  ..., -1.7506e-01,\n",
      "           1.2559e-01,  1.5568e-01],\n",
      "         [-4.7100e-02,  1.9580e-01,  3.3826e-02,  ..., -1.7835e-01,\n",
      "           1.3218e-01,  1.6088e-01],\n",
      "         [-4.7287e-02,  2.0470e-01,  3.5001e-02,  ..., -1.7411e-01,\n",
      "           1.2734e-01,  1.6346e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.6254e-01,  1.6875e-02, -9.6744e-03,  ...,  2.6525e-01,\n",
      "           1.0665e-01,  1.6045e-01],\n",
      "         [-2.6674e-01,  1.3714e-02, -6.5433e-03,  ...,  2.6642e-01,\n",
      "           1.1719e-01,  1.5380e-01],\n",
      "         [-2.6183e-01,  1.9821e-02,  1.1604e-04,  ...,  2.6582e-01,\n",
      "           1.2516e-01,  1.5343e-01],\n",
      "         [-2.6259e-01,  2.0483e-02, -1.0616e-03,  ...,  2.7186e-01,\n",
      "           1.1505e-01,  1.6449e-01]],\n",
      "\n",
      "        [[-2.6877e-01,  7.8975e-02, -2.3230e-03,  ...,  2.1254e-01,\n",
      "           2.4408e-02,  1.9430e-01],\n",
      "         [-2.5495e-01,  9.4679e-02,  1.3766e-02,  ...,  2.1727e-01,\n",
      "           2.5863e-02,  2.0481e-01],\n",
      "         [-2.7102e-01,  7.7350e-02, -2.1474e-03,  ...,  2.1637e-01,\n",
      "           3.3595e-02,  1.9681e-01],\n",
      "         [-2.6816e-01,  8.3522e-02,  3.7350e-03,  ...,  2.1714e-01,\n",
      "           2.6668e-02,  2.0258e-01]],\n",
      "\n",
      "        [[-6.7398e-02,  2.2714e-01,  1.1837e-01,  ...,  4.2263e-02,\n",
      "           4.2057e-02,  2.0017e-01],\n",
      "         [-6.6667e-02,  2.3200e-01,  9.9323e-02,  ...,  3.0060e-02,\n",
      "           3.5438e-02,  2.0123e-01],\n",
      "         [-6.8429e-02,  2.3505e-01,  1.1140e-01,  ...,  2.7685e-02,\n",
      "           3.4532e-02,  2.0033e-01],\n",
      "         [-6.9373e-02,  2.2823e-01,  1.0643e-01,  ...,  2.7097e-02,\n",
      "           4.0357e-02,  2.0644e-01]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([32, 4, 80])\n"
     ]
    }
   ],
   "source": [
    "# n_x, d_x为自注意力的item数量、及对应维度\n",
    "# 矩阵相乘，分别得到变换后的，q,k,v。再使用多头注意力机制就可以\n",
    "n_x = 4\n",
    "d_x = 80\n",
    "x = torch.randn(batch, n_x, d_x) # (batch, 4, 80)\n",
    "mask = torch.zeros(batch, n_x, n_x).bool()\n",
    "selfattn = SelfAttention(n_head=8, d_k=128, d_v=64, d_x=80, d_o=80)\n",
    "attn, output = selfattn(x, mask=mask)\n",
    "    \n",
    "print(attn); print(output)\n",
    "print(attn.size()) # n_head*batch, n_q=n_x, n_k=n_x\n",
    "print(output.size()) # batch, n_q, d_o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
